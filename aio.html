<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Machine Learning in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Machine Learning in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Machine Learning in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Machine Learning in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-what_is_ml.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-data.html">2. Data preparation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-learning.html">3. Learning</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-modelling.html">4. Modelling</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-validation.html">5. Validation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-evaluation.html">6. Evaluation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-bootstrapping.html">7. Bootstrapping</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-leakage.html">8. Data leakage</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li>
<li><a href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-what_is_ml"><p>Content from <a href="01-what_is_ml.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/01-what_is_ml.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is machine learning?</li>
<li>What is the relationship between machine learning, AI, and
statistics?</li>
<li>What is meant by supervised learning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Recognise what is meant by machine learning.</li>
<li>Have an appreciation of the difference between supervised and
unsupervised learning.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="rule-based-programming">Rule-based programming<a class="anchor" aria-label="anchor" href="#rule-based-programming"></a>
</h2>
<hr class="half-width">
<p>We are all familiar with the idea of applying rules to data to gain
insights and make decisions. For example, we learn that human body
temperature is ~37 °C (~98.5 °F), and that higher or lower temperatures
can be cause for concern.</p>
<p>As programmers we understand how to codify these rules. If we were
developing software for a hospital to flag patients at risk of
deterioration, we might create early-warning rules such as those
below:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">def</span> has_fever(temp_c):</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>    <span class="cf">if</span> temp_c <span class="op">&gt;</span> <span class="dv">38</span>:</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="machine-learning">Machine learning<a class="anchor" aria-label="anchor" href="#machine-learning"></a>
</h2>
<hr class="half-width">
<p>With machine learning, we modify this approach. Instead, we give data
<em>and</em> insights to a framework (or “model”) that can learn the
rules for itself. As the volume and complexity of data increases, so
does the value of having models that can generate new rules.</p>
<p>In a 2018 paper entitled “<a href="https://www.nature.com/articles/s41746-018-0029-1" class="external-link">Scalable and
accurate deep learning with electronic health records</a>”, Rajkomar and
colleagues present their work to develop a “deep learning model” (a type
of machine learning model) for predicting hospital mortality. A segment
of the paper’s introduction is reproduced below:</p>
<blockquote>
<p>In spite of the richness and potential of available data [in
healthcare], scaling the development of predictive models is difficult
because, for traditional predictive modeling techniques, each outcome to
be predicted requires the creation of a custom dataset with specific
variables. It is widely held that 80% of the effort in an analytic model
is preprocessing, merging, customizing, and cleaning datasets not
analyzing them for insights. This profoundly limits the scalability of
predictive models.</p>
<p>Another challenge is that the number of potential predictor variables
in the electronic health record (EHR) may easily number in the
thousands, particularly if free-text notes from doctors, nurses, and
other providers are included. Traditional modeling approaches have dealt
with this complexity simply by choosing a very limited number of
commonly collected variables to consider.</p>
<p>… Recent developments in deep learning and artificial neural networks
may allow us to address many of these challenges and unlock the
information in the EHR. … These systems are known for their ability to
handle large volumes of relatively messy data, including errors in
labels and large numbers of input variables. A key advantage is that
investigators do not generally need to specify which potential predictor
variables to consider and in what combinations; instead neural networks
are able to learn representations of the key factors and interactions
from the data itself.</p>
</blockquote>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What is the most time consuming aspect of developing a predictive
model, according to the authors?<br>
</li>
<li>How have “traditional” predictive models dealt with high numbers of
predictor variables, according to the authors?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>80% of effort in building models is in “preprocessing, merging,
customizing, and cleaning”.<br>
</li>
<li>Traditional modeling approaches have dealt with complexity by
choosing a very limited number of variables to consider.</li>
</ol>
</div>
</div>
</div>
</div>
<p>These paragraphs provide an example of how machine learning can help
us with tasks like prediction. They also touch on an area where machine
learning projects often comes under criticism. It is easy to throw tools
at a problem without sufficient thought!</p>
</section><section><h2 class="section-heading" id="statistics-machine-learning-and-ai">Statistics, machine learning, and “AI”<a class="anchor" aria-label="anchor" href="#statistics-machine-learning-and-ai"></a>
</h2>
<hr class="half-width">
<p>There are ongoing and often polarised debates about the relationship
between statistics, machine learning, and “A.I”. There are also plenty
of familiar jokes and memes like this one by <a href="https://www.instagram.com/sandserifcomics/" class="external-link">sandserifcomics</a>.</p>
<figure><img src="fig/section1-fig2.jpeg" alt="Poorly fitted data" width="600" class="figure mx-auto d-block"></figure><p>Keeping out of the fight, a slightly hand-wavy, non-controversial
take might be:</p>
<ul>
<li>
<em>Statistics</em>: A well-established field of mathematics
concerned with methods for collecting, analyzing, interpreting and
presenting empirical data.</li>
<li>
<em>Machine learning</em>: A set of computational methods that learn
rules from data, often with the goal of prediction. Borrows from other
disciplines, notably statistics and computer science.</li>
<li>
<em>Deep learning</em>: A subfield of machine learning that focuses
on more complex “artificial neural network” algorithms.</li>
<li>
<em>Artificial intelligence</em>: The goal of conferring human-like
intelligence to machines. “A.I.” has become popularly used as a synonym
for machine learning, so researchers working on the goal of intelligent
machines have taken to using “Artificial General Intelligence” (A.G.I.)
for clarity.</li>
</ul></section><section><h2 class="section-heading" id="supervised-vs-unsupervised-learning">Supervised vs unsupervised learning<a class="anchor" aria-label="anchor" href="#supervised-vs-unsupervised-learning"></a>
</h2>
<hr class="half-width">
<p>Over the course of four half-day lessons, we will explore key
concepts in machine learning. In this introductory lesson we develop and
evaluate a simple predictive model, touching on some of the core
concepts and techniques that we come across in machine learning
projects.</p>
<p>Our goal will be to become familiar with the kind of language used in
papers such as “<a href="https://www.nature.com/articles/s41746-018-0029-1" class="external-link">Scalable and
accurate deep learning with electronic health records</a>” by Rajkomar
and colleagues.</p>
<p>Our focus will be on <em>supervised</em> machine learning, a category
of machine learning that involves the use of labelled datasets to train
models for classification and prediction. Supervised machine learning
can be contrasted to <em>unsupervised</em> machine learning, which
attempts to identify meaningful patterns within unlabelled datasets.</p>
<p>In later lessons, we take a deeper dive into two popular families of
machine learning models - decision trees and artificial neural networks.
We then explore what it means to be a responsible practitioner of
machine learning.</p>
<div id="exercise-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-1" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>We have laboratory test data on patients admitted to a critical care
unit and we are trying to identify patients with an emerging, rare
disease. There are no labels to indicate which patients have the
disease, but we believe that the infected patients will have very
distinct characteristics. Do we look for a supervised or unsupervised
machine learning approach?<br>
</li>
<li>We would like to predict whether or not patients will respond to a
new drug that is under development based on several genetic markers. We
have a large corpus of clinical trial data that includes both genetic
markers of patients and their response the new drug. Do we use a
supervised or unsupervised approach?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>The prediction targets are not labelled, so an unsupervised learning
approach would be appropriate. Our hope is that we will see a unique
cluster in the data that pertains to the emerging disease.<br>
</li>
<li>We have both genetic markers and known outcomes, so in this case
supervised learning is appropriate.</li>
</ol>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Machine learning borrows heavily from fields such as statistics and
computer science.</li>
<li>In machine learning, models learn rules from data.</li>
<li>In supervised learning, the target in our training data is
labelled.</li>
<li>A.I. has become a synonym for machine learning.</li>
<li>A.G.I. is the loftier goal of achieving human-like
intelligence.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-data"><p>Content from <a href="02-data.html">Data preparation</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/02-data.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why are some common steps in data preparation?</li>
<li>What is SQL and why is it often needed?</li>
<li>What do we partition data at the start of a project?</li>
<li>What is the purpose of setting a random state when
partitioning?</li>
<li>Should we impute missing values before or after partitioning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explore characteristics of our dataset.</li>
<li>Partition data into training and test sets.</li>
<li>Encode categorical values.</li>
<li>Use scaling to pre-process features.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="sourcing-and-accessing-data">Sourcing and accessing data<a class="anchor" aria-label="anchor" href="#sourcing-and-accessing-data"></a>
</h2>
<hr class="half-width">
<p>Machine learning helps us to find patterns in data, so sourcing and
understanding data is key. Unsuitable or poorly managed data will lead
to a poor project outcome, regardless of the modelling approach.</p>
<p>We will be using an open access subset of the <a href="https://eicu-crd.mit.edu/about/eicu/" class="external-link">eICU Collaborative Research
Database</a>, a publicly available dataset comprising deidentified
physiological data collected from critically ill patients. For
simplicity, we will be working with a pre-prepared CSV file that
comprises data extracted from a <a href="https://doi.org/10.13026/4mxk-na84" class="external-link">demo version of the
dataset</a>.</p>
<p>Let’s begin by loading this data:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>cohort <span class="op">=</span> pd.read_csv(<span class="st">'./eicu_cohort.csv'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>cohort.head()</span></code></pre>
</div>
<p>Learning to extract data from sources such as databases and file
systems is a key skill in machine learning. Familiarity with Python and
Structured Query Language (SQL) will equip you well for these tasks. For
reference, the query used to extract the dataset is outlined below.
Briefly, this query:</p>
<ul>
<li>
<code>SELECTs</code> multiple columns</li>
<li>
<code>FROM</code> the <code>patient</code>,
<code>apachepatientresult</code>, and <code>apacheapsvar</code>
tables</li>
<li>
<code>WHERE</code> certain conditions are met.</li>
</ul>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">SQL<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sql" tabindex="0"><code class="sourceCode sql"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">SELECT</span> p.gender, SAFE_CAST(p.age <span class="kw">as</span> int64) <span class="kw">as</span> age, p.admissionweight,</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>       a.unabridgedhosplos, a.acutephysiologyscore, a.apachescore, a.actualhospitalmortality,</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>       av.heartrate, av.meanbp, av.creatinine, av.temperature, av.respiratoryrate,</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>       av.wbc, p.admissionheight</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="kw">FROM</span> `physionet<span class="op">-</span><span class="kw">data</span>.eicu_crd_demo.patient` p</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="kw">INNER</span> <span class="kw">JOIN</span> `physionet<span class="op">-</span><span class="kw">data</span>.eicu_crd_demo.apachepatientresult` a</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="kw">ON</span> p.patientunitstayid <span class="op">=</span> a.patientunitstayid</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="kw">INNER</span> <span class="kw">JOIN</span> `physionet<span class="op">-</span><span class="kw">data</span>.eicu_crd_demo.apacheapsvar` av</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="kw">ON</span> p.patientunitstayid <span class="op">=</span> av.patientunitstayid</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="kw">WHERE</span> apacheversion <span class="kw">LIKE</span> <span class="st">'IVa'</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="knowing-the-data">Knowing the data<a class="anchor" aria-label="anchor" href="#knowing-the-data"></a>
</h2>
<hr class="half-width">
<p>Before moving ahead on a project, it is important to understand the
data. Having someone with domain knowledge - and ideally first hand
knowledge of the data collection process - helps us to design a sensible
task and to use data effectively.</p>
<p>Summarizing data is an important first step. We will want to know
aspects of the data such as: extent of missingness; data types; numbers
of observations. One common step is to view summary characteristics (for
example, see <a href="https://www.nature.com/articles/s41746-018-0029-1/tables/1" class="external-link">Table
1</a> of the paper by Rajkomar et al.).</p>
<p>Let’s generate a similar table for ourselves:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># !pip install tableone</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> tableone <span class="im">import</span> tableone</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># rename columns</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>rename <span class="op">=</span> {<span class="st">"unabridgedhosplos"</span>:<span class="st">"length of stay"</span>,</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>          <span class="st">"meanbp"</span>: <span class="st">"mean blood pressure"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>          <span class="st">"wbc"</span>: <span class="st">"white cell count"</span>}</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="co"># view summary characteristics</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>t1 <span class="op">=</span> tableone(cohort, groupby<span class="op">=</span><span class="st">"actualhospitalmortality"</span>, rename<span class="op">=</span>rename)</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>t1</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="co"># Output to LaTeX</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="co"># print(t1.tabulate(tablefmt = "latex"))</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>|                                 |         | Missing   | Overall      | ALIVE        | EXPIRED      |
|---------------------------------|---------|-----------|--------------|--------------|--------------|
| n                               |         |           | 235          | 195          | 40           |
| gender, n (%)                   | Female  | 0         | 116 (49.4)   | 101 (51.8)   | 15 (37.5)    |
|                                 | Male    |           | 118 (50.2)   | 94 (48.2)    | 24 (60.0)    |
|                                 | Unknown |           | 1 (0.4)      |              | 1 (2.5)      |
| age, mean (SD)                  |         | 9         | 61.9 (15.5)  | 60.5 (15.8)  | 69.3 (11.5)  |
| admissionweight, mean (SD)      |         | 5         | 87.6 (28.0)  | 88.6 (28.8)  | 82.3 (23.3)  |
| length of stay, mean (SD)       |         | 0         | 9.2 (8.6)    | 9.6 (7.5)    | 6.9 (12.5)   |
| acutephysiologyscore, mean (SD) |         | 0         | 59.9 (28.1)  | 54.5 (23.1)  | 86.7 (34.7)  |
| apachescore, mean (SD)          |         | 0         | 71.2 (30.3)  | 64.6 (24.5)  | 103.5 (34.9) |
| heartrate, mean (SD)            |         | 0         | 108.7 (33.1) | 107.9 (30.6) | 112.9 (43.2) |
| mean blood pressure, mean (SD)  |         | 0         | 93.2 (47.0)  | 92.1 (45.4)  | 98.6 (54.5)  |
| creatinine, mean (SD)           |         | 0         | 1.0 (1.7)    | 0.9 (1.7)    | 1.7 (1.6)    |
| temperature, mean (SD)          |         | 0         | 35.2 (6.5)   | 36.1 (3.9)   | 31.2 (12.4)  |
| respiratoryrate, mean (SD)      |         | 0         | 30.7 (15.2)  | 29.9 (15.1)  | 34.3 (15.6)  |
| white cell count, mean (SD)     |         | 0         | 10.5 (8.4)   | 10.7 (8.2)   | 9.7 (9.7)    |
| admissionheight, mean (SD)      |         | 2         | 168.0 (12.8) | 167.7 (13.4) | 169.4 (9.1)  |</code></pre>
</div>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What is the approximate percent mortality in the eICU cohort?<br>
</li>
<li>Which variables appear noticeably different in the “Alive” and
“Expired” groups?<br>
</li>
<li>How does the in-hospital mortality differ between the eICU cohort
and the ones in <a href="https://www.nature.com/articles/s41746-018-0029-1/tables/1" class="external-link">Rajkomar
et al</a>?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>Approximately 17% (40/235)<br>
</li>
<li>Several variables differ, including age, length of stay, acute
physiology score, heart rate, etc.<br>
</li>
<li>The Rajkomar et al dataset has significantly lower in-hospital
mortality (~2% vs 17%).</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="encoding">Encoding<a class="anchor" aria-label="anchor" href="#encoding"></a>
</h2>
<hr class="half-width">
<p>It is often the case that our data includes categorical values. In
our case, for example, the binary outcome we are trying to predict - in
hospital mortality - is recorded as “ALIVE” and “EXPIRED”. Some models
can cope with taking this text as an input, but many cannot. We can use
label encoding to convert the categorical values to numerical
representations.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># check current type</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="bu">print</span>(cohort.dtypes)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># convert to a categorical type</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>categories<span class="op">=</span>[<span class="st">'ALIVE'</span>, <span class="st">'EXPIRED'</span>]</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality'</span>]  <span class="op">=</span> pd.Categorical(cohort[<span class="st">'actualhospitalmortality'</span>], categories<span class="op">=</span>categories)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="co"># add the encoded value to a new column</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality_enc'</span>] <span class="op">=</span> cohort[<span class="st">'actualhospitalmortality'</span>].cat.codes</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>cohort[[<span class="st">'actualhospitalmortality_enc'</span>,<span class="st">'actualhospitalmortality'</span>]].head()</span></code></pre>
</div>
<pre><code>   actualhospitalmortality_enc actualhospitalmortality
0                            0                   ALIVE
1                            0                   ALIVE
2                            0                   ALIVE
3                            1                 EXPIRED
4                            1                 EXPIRED</code></pre>
<p>We’ll encode the gender in the same way:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># convert to a categorical type</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>cohort[<span class="st">'gender'</span>] <span class="op">=</span> pd.Categorical(cohort[<span class="st">'gender'</span>])</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>cohort[<span class="st">'gender'</span>] <span class="op">=</span> cohort[<span class="st">'gender'</span>].cat.codes</span></code></pre>
</div>
<p>Another popular encoding that you will come across in machine
learning is “one hot encoding”. In one hot encoding, categorical
variables are represented as a binary column for each category. The “one
hot” refers to the fact that the category can flip between “hot”
(active, 1) or inactive (0). In the example above,
<code>actualhospitalmortality</code> would be encoded as two columns:
<code>ALIVE</code> and <code>EXPIRED</code>, each containing a list of
0s and 1s.</p>
</section><section><h2 class="section-heading" id="partitioning">Partitioning<a class="anchor" aria-label="anchor" href="#partitioning"></a>
</h2>
<hr class="half-width">
<p>Typically we will want to split our data into a training set and
“held-out” test set. The training set is used for building our model and
our test set is used for evaluation. A split of ~70% training, 30% test
is common.</p>
<figure><img src="fig/train_test.png" alt="Train and test set" width="600" class="figure mx-auto d-block"></figure><p>To ensure reproducibility, we should set the random state of the
splitting method. This means that Python’s random number generator will
produce the same “random” split in future.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>x <span class="op">=</span> cohort.drop(<span class="st">'actualhospitalmortality'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>y <span class="op">=</span> cohort[<span class="st">'actualhospitalmortality'</span>]</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(x, y, train_size <span class="op">=</span> <span class="fl">0.7</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="missing-data">Missing data<a class="anchor" aria-label="anchor" href="#missing-data"></a>
</h2>
<hr class="half-width">
<p>Certain types of models - for example some decision trees - are able
to implicitly handle missing data. For our logistic regression, we will
need to impute values. We will take a simple approach of replacing with
the median.</p>
<p>With physiological data, imputing the median typically implies that
the missing observation is not a cause for concern. In hospital you do
not want to be the interesting patient!</p>
<p>To avoid data leaking between our training and test sets, we take the
median from the training set only. The training median is then used to
impute missing values in the held-out test set.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># impute missing values from the training set</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>x_train <span class="op">=</span> x_train.fillna(x_train.median())</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>x_test <span class="op">=</span> x_test.fillna(x_train.median())</span></code></pre>
</div>
<p>It is often the case that data is not missing at random. For example,
the presence of blood sugar observations may indicate suspected
diabetes. To use this information, we can choose to create missing data
features comprising of binary “is missing” flags.</p>
</section><section><h2 class="section-heading" id="normalisation">Normalisation<a class="anchor" aria-label="anchor" href="#normalisation"></a>
</h2>
<hr class="half-width">
<p>Lastly, normalisation - scaling variables so that they span
consistent ranges - can be important, particularly for models that rely
on distance based optimisation metrics.</p>
<p>As with creating train and test splits, it is a common enough task
that there are plenty of pre-built functions for us to choose from. We
will choose the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" class="external-link">Min-Max
Scaler</a> from the sklearn package, which scales each feature between
zero and one.</p>
<p><span class="math display">\[
x_{std} = \frac{x - x_{min}}{x_{max}-x_{min}}
\]</span></p>
<p><span class="math display">\[
x_{scaled} = x_{std} * (x_{max}-x_{min}) + x_{min}
\]</span></p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># Define the scaler</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co"># Alternative is zero mean, unit variance</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co"># Subtract mean, divide by standard deviation</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co"># from sklearn.preprocessing import StandardScaler</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co"># fit the scaler on the training dataset</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>scaler.fit(x_train)</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="co"># scale the training set</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>x_train <span class="op">=</span> scaler.transform(x_train)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="co"># scale the test set</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>x_test <span class="op">=</span> scaler.transform(x_test)</span></code></pre>
</div>
<p>Outliers in features can have a negative impact on the normalisation
process - they can essentially squash non-outliers into a small space -
so they may need special treatment (for example, a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler" class="external-link">RobustScaler</a>)</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Data pre-processing is arguably the most important task in machine
learning.</li>
<li>SQL is the tool that we use to extract data from database
systems.</li>
<li>Data is typically partitioned into training and test sets.</li>
<li>Setting random states helps to promote reproducibility.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-03-learning"><p>Content from <a href="03-learning.html">Learning</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/03-learning.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do machines learn?</li>
<li>How can machine learning help us to make predictions?</li>
<li>Why is it important to be able to quantify the error in our
models?</li>
<li>What is an example of a loss function?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the importance of quantifying error.</li>
<li>Code a linear regression model that takes inputs, weights, and
bias.</li>
<li>Code a loss function that quantifies model error.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="how-do-machines-learn">How do machines learn?<a class="anchor" aria-label="anchor" href="#how-do-machines-learn"></a>
</h2>
<hr class="half-width">
<p>How do humans learn? Typically we are given examples and we learn
rules through trial and error. Machines aren’t that different! In the
context of machine learning, we talk about how a model “fits” to the
data.</p>
<p>Our model has a number of tweakable parameters. We need to find the
optimal values for those parameters such that our model outputs the
“best” predictions for a set of input variables.</p>
<figure><img src="fig/ml_model.png" alt="Model training" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="loss-functions">Loss functions<a class="anchor" aria-label="anchor" href="#loss-functions"></a>
</h2>
<hr class="half-width">
<p>Finding the best model means defining “best”. We need to have some
way of quantifying the difference between a “good” model (capable of
making useful predictions) vs a “bad” model (not capable of making
useful predictions).</p>
<p>Loss functions are crucial for doing this. They allow us to quantify
how closely our predictions fit to the known target values. You will
hear “objective function”, “error function”, and “cost function” used in
a similar way.</p>
<p>Mean squared error is a common example of a loss function, often used
for linear regression. For each prediction, we measure the distance
between the known target value (<span class="math inline">\(y\)</span>)
and our prediction (<span class="math inline">\(y_{hat}\)</span>), and
then we take the square.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># Create sample labelled data</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>data <span class="op">=</span> {<span class="st">'x'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>], <span class="st">'y'</span>: [<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">7</span>]}</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># Add predictions</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>df[<span class="st">'y_hat'</span>] <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>]</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># plot the data</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>ax <span class="op">=</span> df.plot(x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>, kind<span class="op">=</span><span class="st">'scatter'</span>, xlim<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">6</span>], ylim<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>,<span class="dv">9</span>])</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># plot approx line of best fit</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>ax.plot(df[<span class="st">'x'</span>], df[<span class="st">'y_hat'</span>], color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># plot error</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>ax.vlines(x<span class="op">=</span>df[<span class="st">'x'</span>], ymin<span class="op">=</span>df[<span class="st">'y'</span>], ymax<span class="op">=</span>df[<span class="st">'y_hat'</span>], color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>ax.text(x<span class="op">=</span><span class="fl">3.1</span>, y<span class="op">=</span><span class="dv">3</span>, s<span class="op">=</span><span class="st">'Error'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>ax.set_title(<span class="st">'Prediction error'</span>)</span></code></pre>
</div>
<figure><img src="fig/loss_line_error.png" alt="Distance from target" width="600" class="figure mx-auto d-block"></figure><p>The further away from the data points our line gets, the bigger the
error. Our best model is the one with the smallest error.
Mathematically, we can define the mean squared error as:</p>
<p><span class="math display">\[
mse = \frac{1}{n}\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}
\]</span></p>
<p><span class="math inline">\(mse\)</span> is the Mean Squared Error.
<span class="math inline">\(y_{i}\)</span> is the actual value and <span class="math display">\[\hat{y}_{i}\]</span> is the predicted value.
<span class="math inline">\(\sum_{}\)</span> is notation to indicate
that we are taking the sum of the difference. <span class="math inline">\(n\)</span> is the total number of observations, so
<span class="math display">\[\frac{1}{n}\]</span> indicates that we are
taking the mean.</p>
<p>We could implement this in our code as follows:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="kw">def</span> loss(y, y_hat):</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">    Loss function (mean squared error).</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co">        y (numpy array): The known target values.</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co">        y_hat (numpy array): The predicted values.</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co">        numpy float: The mean squared error.</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    distances <span class="op">=</span> y <span class="op">-</span> y_hat</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>    squared_distances <span class="op">=</span> np.square(distances)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>    <span class="cf">return</span> np.mean(squared_distances)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="minimising-the-error">Minimising the error<a class="anchor" aria-label="anchor" href="#minimising-the-error"></a>
</h2>
<hr class="half-width">
<p>Our goal is to find the “best” model. We have defined best as being
the model with parameters that give us the smallest mean squared error.
We can write this as:</p>
<p><span class="math display">\[
argmin\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}
\]</span></p>
<p>Let’s stop and look at what this loss function means. We’ll plot the
squared error for a range of values to demonstrate how loss scales as
the difference between <span class="math inline">\(y\)</span> and <span class="math display">\[\hat{y}\]</span> increases.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">50</span>, <span class="dv">50</span>, <span class="fl">0.05</span>)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>y <span class="op">=</span> np.square(x)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>plt.xlabel(<span class="st">'Difference between y and y_hat'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss (squared error)'</span>)</span></code></pre>
</div>
<figure><img src="fig/mean_squared_error.png" alt="Mean squared error" width="600" class="figure mx-auto d-block"></figure><p>As we can see, our loss rapidly increases as predictions (<span class="math display">\[\hat{y}\]</span>) move away from the true values
(<span class="math inline">\(y\)</span>). The result is that outliers
have a strong influence on our model fit.</p>
</section><section><h2 class="section-heading" id="optimisation">Optimisation<a class="anchor" aria-label="anchor" href="#optimisation"></a>
</h2>
<hr class="half-width">
<p>In machine learning, there is typically a training step where an
algorithm is used to find the optimal set of model parameters
(i.e. those parameters that give the minimum possible error). This is
the essence of machine learning!</p>
<figure><img src="fig/ml_model_loss.png" alt="Model training" width="600" class="figure mx-auto d-block"></figure><p>There are many approaches to optimisation. <a href="https://en.wikipedia.org/wiki/Gradient_descent" class="external-link">Gradient
descent</a> is a popular approach. In gradient descent we take steps in
the opposite direction of the gradient of a function, seeking the lowest
point (i.e. the lowest error).</p>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What does a loss function quantify?<br>
</li>
<li>What is an example of a loss function?<br>
</li>
<li>What are some other names used for loss functions?<br>
</li>
<li>What is happening when a model is trained?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>A loss function quantifies the goodness of fit of a model (i.e. how
closely its predictions match the known targets).<br>
</li>
<li>One example of a loss function is mean squared error (M.S.E.).<br>
</li>
<li>Objective function, error function, and cost function.<br>
</li>
<li>When a model is trained, we are attempting to find the optimal model
parameters in process known as “optimisation”.</li>
</ol>
</div>
</div>
</div>
</div>
<p>Now that we’ve touched on how machines learn, we’ll tackle the
problem of predicting the outcome of patients admitted to intensive care
units in hospitals across the United States.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Loss functions allow us to define a good model.</li>
<li>
<span class="math inline">\(y\)</span> is a known target. <span class="math inline">\(\hat{y}\)</span> (<span class="math inline">\(y
hat\)</span>) is a prediction.</li>
<li>Mean squared error is an example of a loss function.</li>
<li>After defining a loss function, we search for the optimal solution
in a process known as ‘training’.</li>
<li>Optimisation is at the heart of machine learning.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-04-modelling"><p>Content from <a href="04-modelling.html">Modelling</a></p>
<hr>
<p>Last updated on 2025-02-10 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/04-modelling.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Broadly speaking, when talking about regression and classification,
how does the prediction target differ?</li>
<li>Would linear regression be most useful for a regression or
classification task? How about logistic regression?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use a linear regression model for prediction.</li>
<li>Use a logistic regression model for prediction.</li>
<li>Set a decision boundary to predict an outcome from a
probability.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="regression-vs-classification">Regression vs classification<a class="anchor" aria-label="anchor" href="#regression-vs-classification"></a>
</h2>
<hr class="half-width">
<p>Predicting one or more classes is typically referred to as
<em>classification</em>. The task of predicting a continuous variable on
the other hand (for example, length of hospital stay) is typically
referred to as a <em>regression</em>.</p>
<p>Note that “regression models” can be used for both regression tasks
and classification tasks. Don’t let this throw you off!</p>
<p>We will begin with a linear regression, a type of model borrowed from
statistics that has all of the hallmarks of machine learning (so let’s
call it a machine learning model!), which can be written as:</p>
<p><span class="math display">\[
\hat{y} = wX + b
\]</span></p>
<p>Our predictions can be denoted by <span class="math inline">\(\hat{y}\)</span> (pronounced “y hat”) and our
explanatory variables (or “features”) denoted by <span class="math inline">\(X\)</span>. In our case, we will use a single
feature: the APACHE-IV score, a measure of severity of illness.</p>
<p>There are two parameters of the model that we would like to learn
from the training data: <span class="math inline">\(w\)</span>, weight
and <span class="math inline">\(b\)</span>, bias. Could we use a linear
regression for our classification task? Let’s try fitting a line to our
outcome data.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># import the regression model</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression()</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># use a single feature (apache score)</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># note: remove the reshape if fitting to &gt;1 input variable</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>X <span class="op">=</span> cohort.apachescore.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>y <span class="op">=</span> cohort.actualhospitalmortality_enc.values</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># fit the model to our data</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>reg <span class="op">=</span> reg.fit(X, y)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># get the y values</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="bu">buffer</span> <span class="op">=</span> <span class="fl">0.2</span><span class="op">*</span><span class="bu">max</span>(X)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>X_fit <span class="op">=</span> np.linspace(<span class="bu">min</span>(X) <span class="op">-</span> <span class="bu">buffer</span>, <span class="bu">max</span>(X) <span class="op">+</span> <span class="bu">buffer</span>, num<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>y_fit <span class="op">=</span> reg.predict(X_fit)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>plt.scatter(X, y,  color<span class="op">=</span><span class="st">'black'</span>, marker <span class="op">=</span> <span class="st">'x'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>plt.plot(X_fit, y_fit, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/section5-fig1.png" alt="Linear regression with binary outcome" width="600" class="figure mx-auto d-block"></figure><p>Linear regression places a line through a set of data points that
minimizes the error between the line and the points. It is difficult to
see how a meaningful threshold could be set to predict the binary
outcome in our task. The predicted values can exceed our range of
outcomes.</p>
</section><section><h2 class="section-heading" id="sigmoid-function">Sigmoid function<a class="anchor" aria-label="anchor" href="#sigmoid-function"></a>
</h2>
<hr class="half-width">
<p>The sigmoid function (also known as a logistic function) comes to our
rescue. This function gives an “s” shaped curve that can take a number
and map it into a value between 0 and 1:</p>
<p><span class="math display">\[f : \mathbb{R} \mapsto (0,1)
\]</span></p>
<p>The sigmoid function can be written as:</p>
<p><span class="math display">\[f(x) = \frac{1}{1+e^{-x}}\]</span></p>
<p>Let’s take a look at a curve generated by this function:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">def</span> sigmoid(x, k<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co">    Sigmoid function. </span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">    Adjust k to set slope.</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x <span class="op">/</span> k)) </span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    <span class="cf">return</span> s</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co"># set range of values for x</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>plt.plot(x, sigmoid(x))</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/section5-fig2.png" alt="Sigmoid function" width="600" class="figure mx-auto d-block"></figure><p>We can use this to map our linear regression to produce output values
that fall between 0 and 1.</p>
<p><span class="math display">\[
f(x) = \frac{1}{1+e^{-({wX + b})}}
\]</span></p>
<p>As an added benefit, we can interpret the output value as a
probability. The probability relates to the positive class (the outcome
with value “1”), which in our case is in-hospital mortality
(“EXPIRED”).</p>
</section><section><h2 class="section-heading" id="logistic-regression">Logistic regression<a class="anchor" aria-label="anchor" href="#logistic-regression"></a>
</h2>
<hr class="half-width">
<p>Logistic regressions are powerful models that often outperform more
sophisticated machine learning models. In machine learning studies it is
typical to include performance of a logistic regression model as a
baseline (as they do, for example, in <a href="https://www.nature.com/articles/s41746-018-0029-1#Sec20" class="external-link">Rajkomar
and colleagues</a>).</p>
<p>We need to find the parameters for the best-fitting logistic model
given our data. As before, we do this with the help of a loss function
that quantifies error. Our goal is to find the parameters of the model
that minimise the error. With this model, we no longer use least squares
due to the model’s non-linear properties. Instead we will use log
loss.</p>
</section><section><h2 class="section-heading" id="training-or-fitting-the-model">Training (or fitting) the model<a class="anchor" aria-label="anchor" href="#training-or-fitting-the-model"></a>
</h2>
<hr class="half-width">
<p>As is typically the case when using machine learning packages, we
don’t need to code the loss function ourselves. The function is
implemented as part of our machine learning package (in this case <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" class="external-link">scikit-learn</a>).
Let’s try fitting a Logistic Regression to our data.</p>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Following the previous example for a linear regression, fit a
logistic regression to your data and create a new plot. How do the
predictions differ from before? Hint:
<code>from sklearn.linear_model import LogisticRegression</code>.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>You should see a plot similar to the one below: <img src="fig/section5-fig3.png" alt="Logistic regression" width="600" class="figure">
</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="decision-boundary">Decision boundary<a class="anchor" aria-label="anchor" href="#decision-boundary"></a>
</h2>
<hr class="half-width">
<p>Now that our model is able to output the probability of our outcome,
we can set a decision boundary for the classification task. For example,
we could classify probabilities of &lt; 0.5 as “ALIVE” and &gt;= 0.5 as
“EXPIRED”. Using this approach, we can predict outcomes for a given
input.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>x <span class="op">=</span> [[<span class="dv">90</span>]]</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>outcome <span class="op">=</span> reg.predict(x)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>probs <span class="op">=</span> reg.predict_proba(x)[<span class="dv">0</span>]</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'For x=</span><span class="sc">{</span>x[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, we predict an outcome of "</span><span class="sc">{</span>outcome[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">".</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>      <span class="ss">f'Class probabilities (0, 1): </span><span class="sc">{</span><span class="bu">round</span>(probs[<span class="dv">0</span>],<span class="dv">2</span>)<span class="sc">,</span> <span class="bu">round</span>(probs[<span class="dv">1</span>],<span class="dv">2</span>)<span class="sc">}</span><span class="ss">.'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>For x=90, we predict an outcome of "0".
Class probabilities (0, 1): (0.77, 0.23).</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Linear regression is a popular model for regression tasks.</li>
<li>Logistic regression is a popular model for classification
tasks.</li>
<li>Probabilities that can be mapped to a prediction class.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-05-validation"><p>Content from <a href="05-validation.html">Validation</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/05-validation.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is meant by model accuracy?</li>
<li>What is the purpose of a validation set?</li>
<li>What are two types of cross validation?</li>
<li>What is overfitting?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Train a model to predict patient outcomes on a held-out test
set.</li>
<li>Use cross validation as part of our model training process.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="accuracy">Accuracy<a class="anchor" aria-label="anchor" href="#accuracy"></a>
</h2>
<hr class="half-width">
<p>One measure of the performance of a classification model is accuracy.
Accuracy is defined as the overall proportion of correct predictions.
If, for example, we take 50 shots and 40 of them hit the target, then
our accuracy is 0.8 (40/50).</p>
<figure><img src="fig/japan_ren_hayakawa.jpg" alt="Ren Hayakawa Archery Olympics" width="600" class="figure mx-auto d-block"></figure><p>Accuracy can therefore be defined by the formula below:</p>
<p><span class="math display">\[ Accuracy = \frac{Correct\
predictions}{All\ predictions}\]</span></p>
<p>What is the accuracy of our model at predicting in-hospital
mortality?</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># convert outcome to a categorical type</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>categories<span class="op">=</span>[<span class="st">'ALIVE'</span>, <span class="st">'EXPIRED'</span>]</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality'</span>] <span class="op">=</span> pd.Categorical(cohort[<span class="st">'actualhospitalmortality'</span>], categories<span class="op">=</span>categories)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># add the encoded value to a new column</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality_enc'</span>] <span class="op">=</span> cohort[<span class="st">'actualhospitalmortality'</span>].cat.codes</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>cohort[[<span class="st">'actualhospitalmortality_enc'</span>,<span class="st">'actualhospitalmortality'</span>]].head()</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># define features and outcome</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'apachescore'</span>]</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>outcome <span class="op">=</span> [<span class="st">'actualhospitalmortality_enc'</span>]</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># partition data into training and test sets</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>X <span class="op">=</span> cohort[features]</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>y <span class="op">=</span> cohort[outcome]</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, train_size <span class="op">=</span> <span class="fl">0.7</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co"># restructure data for input into model</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co"># note: remove the reshape if fitting to &gt;1 input variable</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>x_train <span class="op">=</span> x_train.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>y_train <span class="op">=</span> y_train.values.ravel()</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>x_test <span class="op">=</span> x_test.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>y_test <span class="op">=</span> y_test.values.ravel()</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>reg <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>reg.fit(x_train, y_train)</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a><span class="co"># generate predictions</span></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>y_hat_train <span class="op">=</span> reg.predict(x_train)</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>y_hat_test <span class="op">=</span> reg.predict(x_test)</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a><span class="co">#  accuracy on training set</span></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>acc_train <span class="op">=</span> np.mean(y_hat_train <span class="op">==</span> y_train)</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy on training set: </span><span class="sc">{</span>acc_train<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a><span class="co">#  accuracy on test set</span></span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>acc_test <span class="op">=</span> np.mean(y_hat_test <span class="op">==</span> y_test)</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy on test set: </span><span class="sc">{</span>acc_test<span class="sc">:.2f}</span><span class="ss">'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Accuracy on training set: 0.86
Accuracy on test set: 0.82</code></pre>
</div>
<p>Not bad! There was a slight drop in performance on our test set, but
that is to be expected.</p>
</section><section><h2 class="section-heading" id="validation-set">Validation set<a class="anchor" aria-label="anchor" href="#validation-set"></a>
</h2>
<hr class="half-width">
<p>Machine learning is iterative by nature. We want to improve our
model, tuning and evaluating as we go. This leads us to a problem. Using
our test set to iteratively improve our model would be cheating. It is
supposed to be “held out”, not used for training! So what do we do?</p>
<p>The answer is that we typically partition off part of our training
set to use for validation. The “validation set” can be used to
iteratively improve our model, allowing us to save our test set for the
*final* evaluation.</p>
<figure><img src="fig/training_val_set.png" alt="Validation set" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="cross-validation">Cross validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h2>
<hr class="half-width">
<p>Why stop at one validation set? With sampling, we can create many
training sets and many validation sets, each slightly different. We can
then average our findings over the partitions to give an estimate of the
model’s predictive performance</p>
<p>The family of resampling methods used for this is known as “cross
validation”. It turns out that one major benefit to cross validation is
that it helps us to build more robust models.</p>
<p>If we train our model on a single set of data, the model may learn
rules that are overly specific (e.g. “all patients aged 63 years
survive”). These rules will not generalise well to unseen data. When
this happens, we say our model is “overfitted”.</p>
<p>If we train on multiple, subtly-different versions of the data, we
can identify rules that are likely to generalise better outside out
training set, helping to avoid overfitting.</p>
<p>Two popular of the most popular cross-validation methods:</p>
<ul>
<li>K-fold cross validation</li>
<li>Leave-one-out cross validation</li>
</ul></section><section><h2 class="section-heading" id="k-fold-cross-validation">K-fold cross validation<a class="anchor" aria-label="anchor" href="#k-fold-cross-validation"></a>
</h2>
<hr class="half-width">
<p>In K-fold cross validation, “K” indicates the number of times we
split our data into training/validation sets. With 5-fold cross
validation, for example, we create 5 separate training/validation
sets.</p>
<figure><img src="fig/k_fold_cross_val.png" alt="5-fold validation" width="600" class="figure mx-auto d-block"></figure><p>With K-fold cross validation, we select our model to evaluate and
then:</p>
<ol style="list-style-type: decimal">
<li>Partition the training data into a training set and a validation
set. An 80%, 20% split is common.</li>
<li>Fit the model to the training set and make a record of the optimal
parameters.</li>
<li>Evaluate performance on the validation set.</li>
<li>Repeat the process 5 times, then average the parameter and
performance values.</li>
</ol>
<p>When creating our training and test sets, we needed to be careful to
avoid data leaks. The same applies when creating training and validation
sets. We can use a <code>pipeline</code> object to help manage this
issue.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> mean, std</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, RepeatedStratifiedKFold</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># define dataset</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>X <span class="op">=</span> x_train</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>y <span class="op">=</span> y_train</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co"># define the pipeline</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>steps <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>steps.append((<span class="st">'scaler'</span>, MinMaxScaler()))</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>steps.append((<span class="st">'model'</span>, LogisticRegression()))</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="co"># define the evaluation procedure</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>cv <span class="op">=</span> RepeatedStratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, n_repeats<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="co"># evaluate the model using cross-validation</span></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(pipeline, X, y, scoring<span class="op">=</span><span class="st">'accuracy'</span>, cv<span class="op">=</span>cv, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a><span class="co"># report performance</span></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cross-validation accuracy, mean (std): </span><span class="sc">%.2f</span><span class="st"> (</span><span class="sc">%.2f</span><span class="st">)'</span> <span class="op">%</span> (mean(scores)<span class="op">*</span><span class="dv">100</span>, std(scores)<span class="op">*</span><span class="dv">100</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Cross-validation accuracy, mean (std): 81.53 (3.31)</code></pre>
</div>
<p>Leave-one-out cross validation is the same idea, except that we have
many more folds. In fact, we have one fold for each data point. Each
fold we leave out one data point for validation and use all of the other
points for training.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Validation sets are used during model development, allowing models
to be tested prior to testing on a held-out set.</li>
<li>Cross-validation is a resampling technique that creates multiple
validation sets.</li>
<li>Cross-validation can help to avoid overfitting.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-06-evaluation"><p>Content from <a href="06-evaluation.html">Evaluation</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/06-evaluation.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What kind of values go into a confusion matrix?</li>
<li>What do the letters AUROC stand for?</li>
<li>Does an AUROC of 0.5 indicate our predictions were good, bad, or
average?</li>
<li>In the context of evaluating performance of a classifier, what is
TP?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Create a confusion matrix for a predictive model.</li>
<li>Use the confusion matrix to compute popular performance
metrics.</li>
<li>Plot an AUROC curve.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="evaluating-a-classification-task">Evaluating a classification task<a class="anchor" aria-label="anchor" href="#evaluating-a-classification-task"></a>
</h2>
<hr class="half-width">
<p>We trained a machine learning model to predict the outcome of
patients admitted to intensive care units. As there are two outcomes, we
refer to this as a “binary” classification task. We are now ready to
evaluate the model on our held-out test set.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># convert outcome to a categorical type</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>categories<span class="op">=</span>[<span class="st">'ALIVE'</span>, <span class="st">'EXPIRED'</span>]</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality'</span>] <span class="op">=</span> pd.Categorical(cohort[<span class="st">'actualhospitalmortality'</span>], categories<span class="op">=</span>categories)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># add the encoded value to a new column</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality_enc'</span>] <span class="op">=</span> cohort[<span class="st">'actualhospitalmortality'</span>].cat.codes</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>cohort[[<span class="st">'actualhospitalmortality_enc'</span>,<span class="st">'actualhospitalmortality'</span>]].head()</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># define features and outcome</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'apachescore'</span>]</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>outcome <span class="op">=</span> [<span class="st">'actualhospitalmortality_enc'</span>]</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># partition data into training and test sets</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>X <span class="op">=</span> cohort[features]</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>y <span class="op">=</span> cohort[outcome]</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, train_size <span class="op">=</span> <span class="fl">0.7</span>, random_state <span class="op">=</span>  <span class="dv">42</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co"># restructure data for input into model</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co"># note: remove the reshape if fitting to &gt;1 input variable</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>x_train <span class="op">=</span> x_train.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>y_train <span class="op">=</span> y_train.values.ravel()</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>x_test <span class="op">=</span> x_test.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>y_test <span class="op">=</span> y_test.values.ravel()</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>reg <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>reg.fit(x_train, y_train)</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a><span class="co"># generate predictions</span></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>y_hat_test <span class="op">=</span> reg.predict(x_test)</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>y_hat_test_proba <span class="op">=</span> reg.predict_proba(x_test)</span></code></pre>
</div>
<p>Each prediction is assigned a probability of a positive class. For
example, the first 10 probabilities are:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>probs <span class="op">=</span> y_hat_test_proba[:,<span class="dv">1</span>][:<span class="dv">12</span>]</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>rounded_probs <span class="op">=</span> [<span class="bu">round</span>(x,<span class="dv">2</span>) <span class="cf">for</span> x <span class="kw">in</span> probs]</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(rounded_probs)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[0.09, 0.11, 0.23, 0.21, 0.23, 0.21, 0.19, 0.03, 0.2, 0.67, 0.54, 0.72]</code></pre>
</div>
<p>These probabilities correspond to the following predictions, either a
“0” (“ALIVE”) or a 1 (“EXPIRED”):</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(y_hat_test[:<span class="dv">12</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[0 0 0 0 0 0 0 0 0 1 1 1]</code></pre>
</div>
<p>In comparison with the known outcomes, we can put each prediction
into one of the following categories:</p>
<ul>
<li>True positive: we predict “1” (“EXPIRED”) and the true outcome is
“1”.</li>
<li>True negative: we predict “0” (“ALIVE”) and the true outcome is
“0”.</li>
<li>False positive: we predict “1” (“EXPIRED”) and the true outcome is
“0”.</li>
<li>False negative: we predict “0” (“ALIVE”) and the true outcome is
“1”.</li>
</ul>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(y_test[:<span class="dv">12</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[0 0 0 0 0 0 1 0 0 0 0 0]</code></pre>
</div>
</section><section><h2 class="section-heading" id="confusion-matrices">Confusion matrices<a class="anchor" aria-label="anchor" href="#confusion-matrices"></a>
</h2>
<hr class="half-width">
<p>It is common practice to arrange these outcome categories into a
“confusion matrix”, which is a grid that records our predictions against
the ground truth. For a binary outcome, confusion matrices are organised
as follows:</p>
<table class="table">
<colgroup>
<col width="29%">
<col width="34%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="center">Negative (predicted)</th>
<th align="center">Positive (predicted)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Negative (actual)</td>
<td align="center"><strong>TN</strong></td>
<td align="center">FP</td>
</tr>
<tr class="even">
<td align="left">Positive (actual)</td>
<td align="center">FN</td>
<td align="center"><strong>TP</strong></td>
</tr>
</tbody>
</table>
<p>The sum of the cells is the total number of predictions. The diagonal
from top left to bottom right indicates correct predictions. Let’s
visualize the results of the model in the form of a confusion
matrix:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># import the metrics class</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>confusion <span class="op">=</span> metrics.confusion_matrix(y_test, y_hat_test)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>class_names<span class="op">=</span>cohort[<span class="st">'actualhospitalmortality'</span>].cat.categories</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>disp <span class="op">=</span> metrics.ConfusionMatrixDisplay.from_estimator(</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    reg, x_test, y_test, display_labels<span class="op">=</span>class_names,</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    cmap<span class="op">=</span>plt.cm.Blues)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/section7-fig1.png" alt="Confusion matrix" width="600" class="figure mx-auto d-block"></figure><p>We have two columns and rows because we have a binary outcome, but
you can also extend the matrix to plot multi-class classification
predictions. If we had more output classes, the number of columns and
rows would match the number of classes.</p>
</section><section><h2 class="section-heading" id="accuracy">Accuracy<a class="anchor" aria-label="anchor" href="#accuracy"></a>
</h2>
<hr class="half-width">
<p>Accuracy is the overall proportion of correct predictions. Think of a
dartboard. How many shots did we take? How many did we hit? Divide one
by the other and that’s the accuracy.</p>
<p>Accuracy can be written as:</p>
<p><span class="math display">\[
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\]</span></p>
<p>What was the accuracy of our model?</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>acc <span class="op">=</span> metrics.accuracy_score(y_test, y_hat_test)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (model) = </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fu">Accuracy</span> <span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">=</span> <span class="fl">0.82</span></span></code></pre>
</div>
<p>Not bad at first glance. When comparing our performance to guessing
“0” for every patient, however, it seems slightly less impressive!</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>zeros <span class="op">=</span> np.zeros(<span class="bu">len</span>(y_test))</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>acc <span class="op">=</span> metrics.accuracy_score(y_test, zeros)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy (zeros) = </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fu">Accuracy</span> <span class="op">(</span><span class="va">zeros</span><span class="op">)</span> <span class="op">=</span> <span class="fl">0.92</span></span></code></pre>
</div>
<p>The problem with accuracy as a metric is that it is heavily
influenced by prevalence of the positive outcome: because the proportion
of 1s is relatively low, classifying everything as 0 is a safe bet.</p>
<p>We can see that the high accuracy is possible despite totally missing
our target. To evaluate an algorithm in a way that prevalence does not
cloud our assessment, we often look at sensitivity and specificity.</p>
</section><section><h2 class="section-heading" id="sensitivity-a-k-a-recall-and-true-positive-rate">Sensitivity (A.K.A “Recall” and “True Positive Rate”)<a class="anchor" aria-label="anchor" href="#sensitivity-a-k-a-recall-and-true-positive-rate"></a>
</h2>
<hr class="half-width">
<p>Sensitivity is the ability of an algorithm to predict a positive
outcome when the actual outcome is positive. In our case, of the
patients who die, what proportion did we correctly predict? This can be
written as:</p>
<p><span class="math display">\[
Sensitivity = Recall = \frac{TP}{TP+FN}
\]</span></p>
<p>Because a model that calls “1” for everything has perfect
sensitivity, this measure is not enough on its own. Alongside
sensitivity we often report on specificity.</p>
</section><section><h2 class="section-heading" id="specificity-a-k-a-true-negative-rate">Specificity (A.K.A “True Negative Rate”)<a class="anchor" aria-label="anchor" href="#specificity-a-k-a-true-negative-rate"></a>
</h2>
<hr class="half-width">
<p>Specificity relates to the test’s ability to correctly classify
patients who survive their stay (i.e. class “0”). Specificity is the
proportion of those who survive who are predicted to survive. The
formula for specificity is:</p>
<p><span class="math display">\[
Specificity = \frac{TN}{FP+TN}
\]</span></p>
</section><section><h2 class="section-heading" id="receiver-operator-characteristic">Receiver-Operator Characteristic<a class="anchor" aria-label="anchor" href="#receiver-operator-characteristic"></a>
</h2>
<hr class="half-width">
<p>A Receiver-Operator Characteristic (ROC) curve plots 1 - specificity
vs. sensitivity at varying probability thresholds. The area under this
curve is known as the AUROC (or sometimes just the “Area Under the
Curve”, AUC) and it is a well-used measure of discrimination that was
originally developed by radar operators in the 1940s.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>metrics.plot_roc_curve(reg, x_test, y_test)</span></code></pre>
</div>
<p>An AUROC of 0.5 is no better than guessing and an AUROC of 1.0 is
perfect. An AUROC of 0.9 tells us that the 90% of times our model will
assign a higher risk to a randomly selected patient with an event than
to a randomly selected patient without an event</p>
<figure><img src="fig/section7-fig3.png" alt="AUROC" width="600" class="figure mx-auto d-block"></figure><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Confusion matrices are the basis for many popular performance
metrics.</li>
<li>AUROC is the area under the receiver operating characteristic. 0.5
is bad!</li>
<li>TP is True Positive, meaning that our prediction hit its
target.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-07-bootstrapping"><p>Content from <a href="07-bootstrapping.html">Bootstrapping</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/07-bootstrapping.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why do we ‘boot up’ computers?</li>
<li>How is bootstrapping commonly used in machine learning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use bootstrapping to compute confidence intervals.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="bootstrapping">Bootstrapping<a class="anchor" aria-label="anchor" href="#bootstrapping"></a>
</h2>
<hr class="half-width">
<p>In statistics and machine learning, bootstrapping is a resampling
technique that involves repeatedly drawing samples from our source data
with replacement, often to estimate a population parameter. By “with
replacement”, we mean that the same data point may be included in our
resampled dataset multiple times.</p>
<p>The term originates from the impossible idea of lifting ourselves up
without external help, by pulling on our own bootstraps. Side note, but
apparently it’s also why we “boot” up a computer (to run software,
software must first be run, so we bootstrap).</p>
<p>Typically our source data is only a small sample of the ground truth.
Bootstrapping is loosely based on the law of large numbers, which says
that with enough data the empirical distribution will be a good
approximation of the true distribution.</p>
<p>Using bootstrapping, we can generate a distribution of estimates,
rather than a single point estimate. The distribution gives us
information about certainty, or the lack of it.</p>
<p>In <a href="https://www.nature.com/articles/s41746-018-0029-1/figures/2" class="external-link">Figure
2 of the Rajkomar paper</a>, the authors note that “the error bars
represent the bootstrapped 95% confidence interval” for the AUROC
values. Let’s use the same approach to calculate a confidence interval
when evaluating the accuracy of a model on a held-out test set.
Steps:</p>
<ol style="list-style-type: decimal">
<li>Draw a sample of size N from the original dataset with replacement.
This is a bootstrap sample.</li>
<li>Repeat step 1 S times, so that we have S bootstrap samples.</li>
<li>Estimate our value on each of the bootstrap samples, so that we have
S estimates</li>
<li>Use the distribution of estimates for inference (for example,
estimating the confidence intervals).</li>
</ol>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> resample</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># convert outcome to a categorical type</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>categories<span class="op">=</span>[<span class="st">'ALIVE'</span>, <span class="st">'EXPIRED'</span>]</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality'</span>] <span class="op">=</span> pd.Categorical(cohort[<span class="st">'actualhospitalmortality'</span>], categories<span class="op">=</span>categories)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># add the encoded value to a new column</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality_enc'</span>] <span class="op">=</span> cohort[<span class="st">'actualhospitalmortality'</span>].cat.codes</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>cohort[[<span class="st">'actualhospitalmortality_enc'</span>,<span class="st">'actualhospitalmortality'</span>]].head()</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># define features and outcome</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'apachescore'</span>]</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>outcome <span class="op">=</span> [<span class="st">'actualhospitalmortality_enc'</span>]</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co"># partition data into training and test sets</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>X <span class="op">=</span> cohort[features]</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>y <span class="op">=</span> cohort[outcome]</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, train_size <span class="op">=</span> <span class="fl">0.7</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="co"># restructure data for input into model</span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co"># note: remove the reshape if fitting to &gt;1 input variable</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>x_train <span class="op">=</span> x_train.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>y_train <span class="op">=</span> y_train.values.ravel()</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>x_test <span class="op">=</span> x_test.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>y_test <span class="op">=</span> y_test.values.ravel()</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>reg <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>reg.fit(x_train, y_train)</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a><span class="co"># bootstrap predictions</span></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a>accuracy <span class="op">=</span> []</span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>    X_bs, y_bs <span class="op">=</span> resample(x_test, y_test, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>    <span class="co"># make predictions</span></span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>    y_hat <span class="op">=</span> reg.predict(X_bs)</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>    <span class="co"># evaluate model</span></span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a>    score <span class="op">=</span> accuracy_score(y_bs, y_hat)</span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a>    accuracy.append(score)</span></code></pre>
</div>
<p>Let’s plot a distribution of accuracy values computed on the
bootstrap samples.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># plot distribution of accuracy</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>sns.kdeplot(accuracy)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>plt.title(<span class="st">"Accuracy across 1000 bootstrap samples of the held-out test set"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>plt.xlabel(<span class="st">"Accuracy"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/section8-fig1.png" alt="Bootstrapped accuracy" width="600" class="figure mx-auto d-block"></figure><p>We can now take the mean accuracy across the bootstrap samples, and
compute confidence intervals. There are several different approaches to
computing the confidence interval. We will use the percentile method, a
simpler approach that does not require our sampling distribution to be
normally distributed.</p>
</section><section><h2 class="section-heading" id="percentile-method">Percentile method<a class="anchor" aria-label="anchor" href="#percentile-method"></a>
</h2>
<hr class="half-width">
<p>For a 95% confidence interval we can find the middle 95% bootstrap
statistics. This is known as the percentile method. This is the
preferred method because it works regardless of the shape of the
sampling distribution.</p>
<p>Regardless of the shape of the bootstrap sampling distribution, we
can use the percentile method to construct a confidence interval. Using
this method, the 95% confidence interval is the range of points that
cover the middle 95% of bootstrap sampling distribution.</p>
<p>We determine the mean of each sample, call it X̄ , and create the
sampling distribution of the mean. We then take the α/2 and 1 - α/2
percentiles (e.g. the .025<em>1000 and .975</em>1000 = 25th and 975th
bootstrapped statistic), and these are the confidence limits.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># get median</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>median <span class="op">=</span> np.percentile(accuracy, <span class="dv">50</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># get 95% interval</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">100</span><span class="op">-</span><span class="dv">95</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>lower_ci <span class="op">=</span> np.percentile(accuracy, alpha<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>upper_ci <span class="op">=</span> np.percentile(accuracy, <span class="dv">100</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model accuracy is reported on the test set. 1000 bootstrapped samples "</span> </span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>      <span class="ss">f"were used to calculate 95% confidence intervals.</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>      <span class="ss">f"Median accuracy is </span><span class="sc">{</span>median<span class="sc">:.2f}</span><span class="ss"> with a 95% a confidence "</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>      <span class="ss">f"interval of [</span><span class="sc">{</span>lower_ci<span class="sc">:.2f}</span><span class="ss">,</span><span class="sc">{</span>upper_ci<span class="sc">:.2f}</span><span class="ss">]."</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model accuracy is reported on the test set. 1000 bootstrapped samples were used to calculate 95% confidence intervals.
Median accuracy is 0.82 with a 95% a confidence interval of [0.73,0.90].</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sns.kdeplot(accuracy)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>plt.title(<span class="st">"Accuracy across 1000 bootstrap samples of the held-out test set</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>          <span class="st">"showing median with 95</span><span class="ch">\\</span><span class="sc">% c</span><span class="st">onfidence intervals"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>plt.xlabel(<span class="st">"Accuracy"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>plt.axvline(median,<span class="dv">0</span>, <span class="dv">14</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>plt.axvline(lower_ci,<span class="dv">0</span>, <span class="dv">14</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>plt.axvline(upper_ci,<span class="dv">0</span>, <span class="dv">14</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/section8-fig2.png" alt="Bootstrapped accuracy with confidence" width="600" class="figure mx-auto d-block"></figure><p>Once an interval is calculated, it may or may not contain the true
value of the unknown parameter. A 95% confidence level does *not* mean
that there is a 95% probability that the population parameter lies
within the interval.</p>
<p>The confidence interval tells us about the reliability of the
estimation procedure. 95% of confidence intervals computed at the 95%
confidence level contain the true value of the parameter.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Bootstrapping is a resampling technique, sometimes confused with
cross-validation.</li>
<li>Bootstrapping allows us to generate a distribution of estimates,
rather than a single point estimate.</li>
<li>Bootstrapping allows us to estimate uncertainty, allowing
computation of confidence intervals.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-08-leakage"><p>Content from <a href="08-leakage.html">Data leakage</a></p>
<hr>
<p>Last updated on 2025-01-28 |

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/episodes/08-leakage.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are common types of data leakage?</li>
<li>How does data leakage occur?</li>
<li>What are the implications of data leakage?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn to recognise common causes of data leakage.</li>
<li>Understand how data leakage affects models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="data-leakage">Data leakage<a class="anchor" aria-label="anchor" href="#data-leakage"></a>
</h2>
<hr class="half-width">
<p>Data leakage is the mistaken use of information in the model training
process that in reality would not be available at prediction time. The
result of data leakage is overly optimistic expectations and poor
performance in out-of-sample datasets.</p>
<p>An extreme example of data leakage would be accidentally including a
prediction target in a training dataset. In this case our model would
perform very well on training data. The model would fail, however, when
moved to a real-life setting where the outcome was not available at
prediction time.</p>
<p>In most cases information leakage is much more subtle than including
the outcome in the training data, and it may occur at many stages during
the model training process.</p>
</section><section><h2 class="section-heading" id="subset-contamination">Subset contamination<a class="anchor" aria-label="anchor" href="#subset-contamination"></a>
</h2>
<hr class="half-width">
<p>It is common to impute missing values, for example by replacing
missing values with the mean or median. Similarly, data is often
normalised by dividing through by the average or maximum.</p>
<p>If these steps are done using the full dataset (i.e. the training and
testing data), then information about the testing data will “leak” into
the training data. The result is likely to be overoptimistic performance
on the test set. For this reason, imputation and normalisation should be
done on subsets independently.</p>
<p>Another issue that can lead to data leakage is to not account for
grouping within a dataset when creating train-test splits. Let’s say,
for example, that we are trying to use chest x-rays to predict which
patients have cardiac disease. If the same patient appears multiple
times within our dataset and this patient appears in both our training
and test set, this may be cause for concern.</p>
<figure><img src="fig/xray-split.png" alt="Dataset leakage" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="target-leakage">Target leakage<a class="anchor" aria-label="anchor" href="#target-leakage"></a>
</h2>
<hr class="half-width">
<p>Target leakage occurs when a prediction target is inadvertently used
in the training process.</p>
<p>The following abstract is taken from <a href="https://www.jmir.org/2018/1/e22/PDF" class="external-link">a 2018 paper</a> entitled:
“Prediction of Incident Hypertension Within the Next Year: Prospective
Study Using Statewide Electronic Health Records and Machine
Learning”:</p>
<blockquote>
<p><em>Background: As a high-prevalence health condition, hypertension
is clinically costly, difficult to manage, and often leads to severe and
life-threatening diseases such as cardiovascular disease (CVD) and
stroke.</em></p>
<p><em>Objective: The aim of this study was to develop and validate
prospectively a risk prediction model of incident essential hypertension
within the following year.</em></p>
<p><em>Methods: Data from individual patient electronic health records
(EHRs) were extracted from the Maine Health Information Exchange
network. Retrospective (N=823,627, calendar year 2013) and prospective
(N=680,810, calendar year 2014) cohorts were formed. A machine learning
algorithm, XGBoost, was adopted in the process of feature selection and
model building. It generated an ensemble of classification trees and
assigned a final predictive risk score to each individual.</em></p>
<p><em>Results: The 1-year incident hypertension risk model attained
areas under the curve (AUCs) of 0.917 and 0.870 in the retrospective and
prospective cohorts, respectively. Risk scores were calculated and
stratified into five risk categories, with 4526 out of 381,544 patients
(1.19%) in the lowest risk category (score 0-0.05) and 21,050 out of
41,329 patients (50.93%) in the highest risk category (score 0.4-1)
receiving a diagnosis of incident hypertension in the following 1 year.
Type 2 diabetes, lipid disorders, CVDs, mental illness, clinical
utilization indicators, and socioeconomic determinants were recognized
as driving or associated features of incident essential hypertension.
The very high risk population mainly comprised elderly (age&gt;50 years)
individuals with multiple chronic conditions, especially those receiving
medications for mental disorders. Disparities were also found in social
determinants, including some community-level factors associated with
higher risk and others that were protective against
hypertension.</em></p>
<p><em>Conclusions: With statewide EHR datasets, our study prospectively
validated an accurate 1-year risk prediction model for incident
essential hypertension. Our real-time predictive analytic model has been
deployed in the state of Maine, providing implications in interventions
for hypertension and related diseases and hopefully enhancing
hypertension care.</em></p>
</blockquote>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What is the prediction target?<br>
</li>
<li>What kind of algorithm is used in the study?<br>
</li>
<li>What performance metric is reported in the results?<br>
</li>
<li>How many features were included in the model? (Hint: see Appendix 3
in the paper)</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>The prediction target is “hypertension within the following
year.”<br>
</li>
<li>The study uses XGBoost, a tree based model.<br>
</li>
<li>The abstract reports AUC (Area under the Receiver Operating
Characteristic Curve.<br>
</li>
<li>Appendix 3 includes a list of features. There are 80 in total.</li>
</ol>
</div>
</div>
</div>
</div>
<p>In <a href="(https://www.jmir.org/2018/1/e22/PDF)" class="external-link">a subsequent
paper</a>, entitled Data Leakage in Health Outcomes Prediction With
Machine Learning Chiavegatto, Filho et al reflect on the previous study.
The abstract is copied below:</p>
<blockquote>
<p><em>The objective of the study was to “develop and validate
prospectively a risk prediction model of incident essential hypertension
within the following year.” The authors follow good prediction protocols
by applying a high-performing machine learning algorithm (XGBoost) and
by validating the results on unseen data from the following year. The
algorithm attained a very high area under the curve (AUC) value of 0.870
for incidence prediction of hypertension in the following year.</em></p>
<p><em>The authors follow this impressive result by commenting on some
of the most important predictive variables, such as demographic
features, diagnosed chronic diseases, and mental illness. The ranking of
the variables that were most important for the predictive performance of
hypertension is included in a multimedia appendix; however, the
above-mentioned variables are not listed near the top. Of the six most
important variables, five were: lisinopril, hydrochlorothiazide,
enalapril maleate, amlodipine besylate, and losartan potassium. All of
these are popular antihypertensive drugs.</em></p>
<p><em>By including the use of antihypertensive drugs as predictors for
hypertension incidence in the following year, Dr Ye and colleagues’work
opens the possibility that the machine learning algorithm will focus on
predicting those already with hypertension but did not have this
information on their medical record at baseline. … just one variable
(the use of a hypertension drug) is sufficient for physicians to infer
the presence of hypertension.</em></p>
</blockquote>
<div id="exercise-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-1" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What are lisinopril, hydrochlorothiazide, enalapril maleate,
amlodipine besylate, and losartan potassium?<br>
</li>
<li>Why is it problematic that these drugs are included as features in
the model?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>They are drugs that are prescribed to people with
hypertension.<br>
</li>
<li>The fact that patients were taking the drugs suggests that
hypertension was already known.</li>
</ol>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Leakage occurs when training data is contaminated with information
that is not available at prediction time.</li>
<li>Leakage leads to over-optimistic expectations of performance.</li>
</ul>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/machine-learning-novice-python/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/machine-learning-novice-python/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:tpollard@mit.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.11" class="external-link">sandpaper (0.16.11)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/machine-learning-novice-python/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/machine-learning-novice-python/aio.html",
  "identifier": "https://carpentries-incubator.github.io/machine-learning-novice-python/aio.html",
  "dateCreated": "2021-10-15",
  "dateModified": "2025-02-11",
  "datePublished": "2025-02-11"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

