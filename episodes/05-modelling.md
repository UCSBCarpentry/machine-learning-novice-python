---
title: "Modelling"
teaching: 20
exercises: 20
questions:
- "How do we build a classifier?"
objectives:
- "Build a model for predicting patient outcomes."
keypoints:
- "Logistic regression is a popular model for classification."
---

## Regression vs classification

In our previous example, we predicted blood pH from pCO2 using linear regression. The task of predicting a continuous variable is called a "regression". Recall that the function for a linear regression is:

$$
f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_n x_n
$$

where $$f(x)$$ is our outcome, $$\beta_0$$, $$\beta_0$$, $$\beta_0$$ are our model parameters, and $$x_1$$, $$x_2$$, $$x_n$$ are our explanatory variables (or, in machine learning, "features").

As a reminder, our new goal is to predict the outcome of hospital patients. Tasks to predict one or more classes are typically called "classification". Could we use a linear regression for our classification task? Let's try fitting a line to our outcome data.

```python
# import the regression model
from sklearn.linear_model import LinearRegression
reg = LinearRegression()

# use a single feature (apache score)
X = cohort.apachescore.values.reshape((len(cohort.apachescore.values), 1))
y = cohort.actualhospitalmortality_enc.values

# fit the model to our data
reg = reg.fit(X, y)

# get the y values
buffer = 0.2*max(X)
X_fit = np.linspace(min(X) - buffer, max(X) + buffer, num=50)
y_fit = reg.predict(X_fit)

# plot
plt.scatter(X, y,  color='black', marker = 'x')
plt.plot(X_fit, y_fit, color='red', linewidth=2)
plt.show()
```
{: .language-python}

![Linear regression with binary outcome](../fig/section5-fig1.png){: width="600px"}

Linear regression is used to place a line through a set of data points that minimizes error between the line and the points. It is difficult to see how a meaningful threshold could be set to predict the binary outcome in our task. The predicted values can exceed our range of outcomes.

## Sigmoid function

The sigmoid function (also known as a logistic function) comes to our rescue. This function gives an "s" shaped curve that can take a number and map it into a value between 0 and 1: 

$$f : \mathbb{R} \mapsto (0,1) $$ 

The sigmoid function can be written as:

$$f(x) = \frac{1}{1+e^{-x}}$$

Let's take a look at a curve generated by this function:

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x, k=0.1):
    """
    Sigmoid function. 
    Adjust k to set slope.
    """
    s = 1 / (1 + np.exp(-x / k)) 
    return s

# set range of values for x
x = np.linspace(-1, 1, 50)

plt.plot(x, sigmoid(x))
plt.show()
```

![Sigmoid function](../fig/section5-fig2.png){: width="600px"}

We can use this to map our linear regression to produce output values that fall between 0 and 1.

$$
f(x) = \frac{1}{1+e^{-({\beta_0 + \beta_n x_n})}}
$$

As an added benefit, we can interpret the output value as a probability. The probability relates to the positive class (the outcome with value "1"), which in our case is in-hospital mortality ("EXPIRED").

## Loss function

We need to find the parameters for the best-fitting logistic model given our data. As before, we do this by defining a loss function that quantifies error. Our goal is to find the parameters of the model that minimise the error.

With this model, we can no longer use least squares due to its non-linear properties. Instead we compute the maximum likelihood estimate (MLE). We are quantifying our model error so that we can search our parameter space for the optimal model.

Let's try again, this time with Logistic Regression.

```python
# import the regression model
from sklearn.linear_model import LogisticRegression
reg = LogisticRegression(random_state=0)

# use a single feature (apache score)
X = cohort.apachescore.values.reshape((len(cohort.apachescore.values), 1))
y = cohort.actualhospitalmortality_enc.values

# fit the model to our data
reg = reg.fit(X, y)

# get the y values
buffer = 0.2*max(X)
X_fit = np.linspace(min(X) - buffer, max(X) + buffer, num=50)
y_fit = reg.predict(X_fit)

# plot
plt.scatter(X, y,  color='black', marker = 'x')
plt.plot(X_fit, y_fit, color='red', linewidth=2)
plt.show()
```

![Logistic regression](../fig/section5-fig3.png){: width="600px"}

## Decision boundary

Now that our model is able to output the probability of our outcome, we can set a decision boundary for the classification task. For example, we could classify probabilities of < 0.5 as "ALIVE" and >= 0.5 as "EXPIRED". Using this approach, we can predict outcomes for a given input.

```python
x = [[90]]
outcome = reg.predict(x)
probs = reg.predict_proba(x)[0]
print(f'For x={x[0][0]}, we predict an outcome of "{outcome[0]}".\n'
      f'Class probabilities (0, 1): {round(probs[0],2), round(probs[1],2)}.')
```

```
For x=90, we predict an outcome of "0".
Class probabilities (0, 1): (0.61, 0.39).
```
{: .output}

{% include links.md %}